{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"df91094d076546c08bbcd0eca4ada6fb\"\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-12-01-preview'\n",
    "os.environ['AZURE_DEPLOYMENT'] = 'gpt-35-turbo-20240510'\n",
    "os.environ['AZURE_ENDPOINT'] = 'https://openai-20240510.openai.azure.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 20:02:12,947 - Load pretrained SentenceTransformer: ./models/multi-qa-mpnet-base-cos-v1\n",
      "2024-08-06 20:02:13,365 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from raptor.EmbeddingModels import SBertEmbeddingModel\n",
    "\n",
    "embedding_model = SBertEmbeddingModel(\n",
    "    \"./models/multi-qa-mpnet-base-cos-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-06 20:02:08,896 - Loading faiss with AVX2 support.\n",
      "2024-08-06 20:02:08,910 - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "from raptor.SummarizationModels import AzureGPTSummarizationModel\n",
    "\n",
    "summarization_model = AzureGPTSummarizationModel(\n",
    "    'gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor.QAModels import AzureGPTQAModel\n",
    "\n",
    "qa_model = AzureGPTQAModel(\n",
    "    'gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 20:02:16,422 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.AzureGPTSummarizationModel object at 0x000001EBA7353790>\n",
      "            Embedding Models: {'EMB': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001EBA72B5310>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-08-06 20:02:16,423 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.AzureGPTSummarizationModel object at 0x000001EBA7353790>\n",
      "            Embedding Models: {'EMB': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001EBA72B5310>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-08-06 20:02:16,423 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.AzureGPTSummarizationModel object at 0x000001EBA7353790>\n",
      "            Embedding Models: {'EMB': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001EBA72B5310>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001EBA72B5310>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <raptor.QAModels.GPT3TurboQAModel object at 0x000001EBA8F36FD0>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from raptor import RetrievalAugmentation, RetrievalAugmentationConfig\n",
    "\n",
    "ra_config = RetrievalAugmentationConfig(\n",
    "    embedding_model=embedding_model,\n",
    "    summarization_model=summarization_model,\n",
    "    qa_model=qa_model,\n",
    ")\n",
    "# Initialize with default configuration. For advanced configurations, check the documentation. [WIP]\n",
    "RA = RetrievalAugmentation(\n",
    "    ra_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 20:02:36,717 - Creating Leaf Nodes\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.09it/s]\n",
      "2024-08-06 20:02:42,993 - Created 35 Leaf Embeddings\n",
      "2024-08-06 20:02:42,993 - Building All Nodes\n",
      "2024-08-06 20:02:42,994 - Using Cluster TreeBuilder\n",
      "2024-08-06 20:02:42,994 - Constructing Layer 0\n",
      "e:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "2024-08-06 20:02:50,018 - Summarization Length: 100\n",
      "2024-08-06 20:02:53,303 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-06 20:02:53,305 - Node Texts Length: 475, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "2024-08-06 20:02:56,501 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-06 20:02:56,503 - Node Texts Length: 407, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.67it/s]\n",
      "2024-08-06 20:02:59,602 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-06 20:02:59,604 - Node Texts Length: 384, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\n",
      "2024-08-06 20:03:03,024 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-06 20:03:03,026 - Node Texts Length: 481, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "2024-08-06 20:03:05,801 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-06 20:03:05,802 - Node Texts Length: 188, Summarized Text Length: 78\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.54it/s]\n",
      "2024-08-06 20:03:08,726 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-06 20:03:08,728 - Node Texts Length: 489, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "2024-08-06 20:03:11,748 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-06 20:03:11,749 - Node Texts Length: 589, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "2024-08-06 20:03:15,330 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-06 20:03:15,332 - Node Texts Length: 268, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "2024-08-06 20:03:15,517 - Constructing Layer 1\n",
      "2024-08-06 20:03:15,518 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 1\n",
      "2024-08-06 20:03:15,518 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001EBA72B5310>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "with open('./demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 20:03:28,754 - Using collapsed_tree\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.81it/s]\n",
      "2024-08-06 20:03:31,288 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Cinderella reached her happy ending by attending the king's festival with the help of a little white bird who granted her wishes, wearing a beautiful dress and slippers thrown down to her by the bird, and capturing the heart of the prince who danced with her all night. The prince searched for her and found her, and they were married with the help of two white doves who announced that she was the true bride.\n"
     ]
    }
   ],
   "source": [
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\yi\\NUSTM\\project\\FinQA4\\raptor\\models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\n",
      "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]Downloading '.gitattributes' to 'models\\.cache\\huggingface\\download\\.gitattributes.8d1395b12188a31e8a22acea211f312d818744e9.incomplete'\n",
      "Downloading 'model.safetensors' to 'models\\.cache\\huggingface\\download\\model.safetensors.7b42b8f6259f5a9b83c5d0acd7cfde8f6fa9833a2edd3f8da4a94da7a03fa037.incomplete'\n",
      "Downloading 'data_config.json' to 'models\\.cache\\huggingface\\download\\data_config.json.a3294881c9834ac6fb99d420008f30b741fe7e86.incomplete'\n",
      "Downloading 'config.json' to 'models\\.cache\\huggingface\\download\\config.json.b9fd4298819da011007a6a4ceb728c860914fc88.incomplete'\n",
      "Downloading 'config_sentence_transformers.json' to 'models\\.cache\\huggingface\\download\\config_sentence_transformers.json.fd1b291129c607e5d49799f87cb219b27f98acdf.incomplete'\n",
      "Downloading '1_Pooling/config.json' to 'models\\.cache\\huggingface\\download\\1_Pooling\\config.json.4e09f293dfe90bba49f87cfe7996271f07be2666.incomplete'\n",
      "Downloading 'README.md' to 'models\\.cache\\huggingface\\download\\README.md.a1dc10aed0e6a7a29bc1bfcf9a833fbc7aae659f.incomplete'\n",
      "Downloading 'modules.json' to 'models\\.cache\\huggingface\\download\\modules.json.952a9b81c0bfd99800fabf352f69c7ccd46c5e43.incomplete'\n",
      "Download complete. Moving file to models\\.gitattributes\n",
      "\n",
      "Fetching 15 files:   7%|▋         | 1/15 [00:01<00:27,  1.97s/it]Download complete. Moving file to models\\data_config.json\n",
      "Download complete. Moving file to models\\config.json\n",
      "Download complete. Moving file to models\\config_sentence_transformers.json\n",
      "Download complete. Moving file to models\\1_Pooling\\config.json\n",
      "Download complete. Moving file to models\\README.md\n",
      "Download complete. Moving file to models\\modules.json\n",
      "Downloading 'pytorch_model.bin' to 'models\\.cache\\huggingface\\download\\pytorch_model.bin.5aba022a15fe19a16a4a78271c9289705066308dbf65765618cc7f4856bcd582.incomplete'\n",
      "Downloading 'sentence_bert_config.json' to 'models\\.cache\\huggingface\\download\\sentence_bert_config.json.f789d99277496b282d19020415c5ba9ca79ac875.incomplete'\n",
      "Downloading 'special_tokens_map.json' to 'models\\.cache\\huggingface\\download\\special_tokens_map.json.378d4fa393d5eaccf69c437a20f1cda6ac65c14d.incomplete'\n",
      "Downloading 'tokenizer.json' to 'models\\.cache\\huggingface\\download\\tokenizer.json.99ab0363136b2901455b583971ad62829c311cfe.incomplete'\n",
      "Downloading 'train_script.py' to 'models\\.cache\\huggingface\\download\\train_script.py.9f4d21eac22ab68ce4267c7a4a9daa1f54d388c0.incomplete'\n",
      "Downloading 'tokenizer_config.json' to 'models\\.cache\\huggingface\\download\\tokenizer_config.json.20ae1276042f43d1c80f4f7b7f084a8704592c1d.incomplete'\n",
      "Downloading 'vocab.txt' to 'models\\.cache\\huggingface\\download\\vocab.txt.1c51ab79a2298a340952d3e6012042a9c84bbe4d.incomplete'\n",
      "Download complete. Moving file to models\\sentence_bert_config.json\n",
      "Download complete. Moving file to models\\special_tokens_map.json\n",
      "Download complete. Moving file to models\\train_script.py\n",
      "Download complete. Moving file to models\\tokenizer_config.json\n",
      "Download complete. Moving file to models\\tokenizer.json\n",
      "Download complete. Moving file to models\\vocab.txt\n",
      "Download complete. Moving file to models\\pytorch_model.bin\n",
      "Download complete. Moving file to models\\model.safetensors\n",
      "\n",
      "Fetching 15 files:  47%|████▋     | 7/15 [00:41<00:48,  6.08s/it]\n",
      "Fetching 15 files: 100%|██████████| 15/15 [00:41<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
    "!huggingface-cli download --resume-download sentence-transformers/multi-qa-mpnet-base-cos-v1 --local-dir ./models/multi-qa-mpnet-base-cos-v1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
