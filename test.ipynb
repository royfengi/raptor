{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"df91094d076546c08bbcd0eca4ada6fb\"\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-12-01-preview'\n",
    "os.environ['AZURE_DEPLOYMENT'] = 'gpt-35-turbo-20240510'\n",
    "os.environ['AZURE_ENDPOINT'] = 'https://openai-20240510.openai.azure.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-07 18:44:03,722 - Loading faiss with AVX2 support.\n",
      "2024-08-07 18:44:03,737 - Successfully loaded faiss with AVX2 support.\n",
      "2024-08-07 18:44:04,132 - Load pretrained SentenceTransformer: ./models/multi-qa-mpnet-base-cos-v1\n",
      "2024-08-07 18:44:04,560 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from raptor.EmbeddingModels import SBertEmbeddingModel\n",
    "\n",
    "embedding_model = SBertEmbeddingModel(\n",
    "    \"./models/multi-qa-mpnet-base-cos-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor.SummarizationModels import AzureGPTSummarizationModel\n",
    "\n",
    "summarization_model = AzureGPTSummarizationModel(\n",
    "    'gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from raptor.QAModels import AzureGPTQAModel, UnifiedQAModel\n",
    "\n",
    "# qa_model = AzureGPTQAModel(\n",
    "#     'gpt-3.5-turbo'\n",
    "# )\n",
    "\n",
    "qa_model = UnifiedQAModel(\n",
    "    R\"./models/unifiedqa-v2-t5-3b-1363200\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'iron'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model.answer_question(\n",
    "    '',\n",
    "    'which is best conductor? \\\\n (a) iron (b) feather'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 18:44:34,902 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.AzureGPTSummarizationModel object at 0x000001D1C4808110>\n",
      "            Embedding Models: {'EMB': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001D1CFB8CD50>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-08-07 18:44:34,903 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.AzureGPTSummarizationModel object at 0x000001D1C4808110>\n",
      "            Embedding Models: {'EMB': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001D1CFB8CD50>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-08-07 18:44:34,904 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.AzureGPTSummarizationModel object at 0x000001D1C4808110>\n",
      "            Embedding Models: {'EMB': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001D1CFB8CD50>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x000001D1CFB8CD50>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <raptor.QAModels.UnifiedQAModel object at 0x000001D1D0E0D250>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from raptor import RetrievalAugmentation, RetrievalAugmentationConfig\n",
    "\n",
    "ra_config = RetrievalAugmentationConfig(\n",
    "    embedding_model=embedding_model,\n",
    "    summarization_model=summarization_model,\n",
    "    qa_model=qa_model,\n",
    ")\n",
    "# Initialize with default configuration. For advanced configurations, check the documentation. [WIP]\n",
    "RA = RetrievalAugmentation(\n",
    "    ra_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 20:45:39,291 - Creating Leaf Nodes\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "2024-08-07 20:45:45,423 - Created 35 Leaf Embeddings\n",
      "2024-08-07 20:45:45,424 - Building All Nodes\n",
      "2024-08-07 20:45:59,762 - Using Cluster TreeBuilder\n",
      "2024-08-07 20:45:59,762 - Constructing Layer 0\n",
      "2024-08-07 20:46:04,667 - Summarization Length: 100\n",
      "2024-08-07 20:46:07,352 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-07 20:46:31,190 - Node Texts Length: 765, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\n",
      "2024-08-07 20:46:34,059 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-07 20:46:38,978 - Node Texts Length: 407, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "2024-08-07 20:46:41,986 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./demo/sample.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mRA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\raptor\\RetrievalAugmentation.py:219\u001b[0m, in \u001b[0;36mRetrievalAugmentation.add_documents\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;66;03m# self.add_to_existing(docs)\u001b[39;00m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;241m=\u001b[39m TreeRetriever(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_retriever_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree)\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\raptor\\tree_builder.py:291\u001b[0m, in \u001b[0;36mTreeBuilder.build_from_text\u001b[1;34m(self, text, use_multithreading)\u001b[0m\n\u001b[0;32m    287\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding All Nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m all_nodes \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(leaf_nodes)\n\u001b[1;32m--> 291\u001b[0m root_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_to_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m tree \u001b[38;5;241m=\u001b[39m Tree(all_nodes, root_nodes, leaf_nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, layer_to_nodes)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\raptor\\cluster_tree_builder.py:130\u001b[0m, in \u001b[0;36mClusterTreeBuilder.construct_tree\u001b[1;34m(self, current_level_nodes, all_tree_nodes, layer_to_nodes, use_multithreading)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m clusters:\n\u001b[1;32m--> 130\u001b[0m         \u001b[43mprocess_cluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnew_level_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnext_node_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43msummarization_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m         next_node_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    139\u001b[0m layer_to_nodes[layer \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(new_level_nodes\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\raptor\\cluster_tree_builder.py:77\u001b[0m, in \u001b[0;36mClusterTreeBuilder.construct_tree.<locals>.process_cluster\u001b[1;34m(cluster, new_level_nodes, next_node_index, summarization_length, lock)\u001b[0m\n\u001b[0;32m     69\u001b[0m node_texts \u001b[38;5;241m=\u001b[39m get_text(cluster)\n\u001b[0;32m     71\u001b[0m summarized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummarize(\n\u001b[0;32m     72\u001b[0m     context\u001b[38;5;241m=\u001b[39mnode_texts,\n\u001b[0;32m     73\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39msummarization_length,\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     76\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode Texts Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(node_texts))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Summarized Text Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummarized_text\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m __, new_parent_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[0;32m     81\u001b[0m     next_node_index, summarized_text, {node\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m cluster}\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lock:\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\tiktoken\\core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "with open('./demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{35: <raptor.tree_structures.Node at 0x1d260b7e6d0>,\n",
       " 36: <raptor.tree_structures.Node at 0x1d25c42c2d0>,\n",
       " 37: <raptor.tree_structures.Node at 0x1d2613ca8d0>,\n",
       " 38: <raptor.tree_structures.Node at 0x1d261337410>,\n",
       " 39: <raptor.tree_structures.Node at 0x1d261346450>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RA.tree.root_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong\n",
    "node_texts = 'back-door into the garden, and called, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick      the good into the pot,      the bad into the crop Then two white pigeons came in by the kitchen window, and afterwards the turtle-doves, and at last all the birds beneath the sky, came whirring and crowding in, and alighted amongst the ashes\\n\\nAnd the pigeons nodded with their heads and began pick, pick, pick, pick, and the rest began also pick, pick, pick, pick, and gathered all the good grains into the dish   Hardly had one hour passed before they had finished, and all flew out again   Then the girl took the dish to her step-mother, and was glad, and believed that now she would be allowed to go with them to the festival\\n\\ndishes of lentils amongst the ashes, the maiden went through the back-door into the garden and cried, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick      the good into the pot,      the bad into the crop Then two white pigeons came in by the kitchen-window, and afterwards the turtle-doves, and at length all the birds beneath the\\n\\nsky, came whirring and crowding in, and alighted amongst the ashes   And the doves nodded with their heads and began pick, pick, pick, pick, and the others began also pick, pick, pick, pick, and gathered all the good seeds into the dishes, and before half an hour was over they had already finished, and all flew out again Then the maiden was delighted, and believed that she might now go with them to the wedding\\n\\nthe left, and the younger at the right, and then the pigeons pecked out the other eye from each   And thus, for their wickedness and falsehood, they were punished with blindness all their days\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text = \"Cinderella goes to sit under a hazel tree three times a day to weep and pray. A little white bird always appears on the tree when Cinderella expresses a wish, and throws down to her what she had wished for. The king gave orders for a festival where all the beautiful young girls in the country were invited so that his son could choose a bride. Cinderella's step-sisters ask her to comb their hair, brush their shoes, and fasten their buckles for the\"\n",
    "node_texts = \" Thrice a day cinderella went and sat beneath it, and wept and prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride\\n\\n  When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so\\n\\nthe step-sisters had gone once more, cinderella went to the hazel-tree and said -      shiver and quiver, my little tree,      silver and gold throw down over me Then the bird threw down a much more beautiful dress than on the preceding day  And when cinderella appeared at the wedding in this dress, every one was astonished at her beauty   The king's son had waited until she came, and instantly took her by the hand\\n\\nAnd now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden   And when she went to the festival in the dress, no one knew how to speak for astonishment   The king's son danced with her only, and if any one invited her to dance, he said this is my partner When evening came, cinderella wished to leave, and the king's\\n\\nhowever, took cinderella on his horse and rode away with her   As they passed by the hazel-tree, the two white doves cried -      turn and peep, turn and peep,      no blood is in the shoe,      the shoe is not too small for her,      the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right,\\n\\nthe other on the left, and remained sitting there When the wedding with the king's son was to be celebrated, the two false sisters came and wanted to get into favor with cinderella and share her good fortune   When the betrothed couple went to church, the elder was at the right side and the younger at the left, and the pigeons pecked out one eye from each of them   Afterwards as they came back the elder was at\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 20:03:28,754 - Using collapsed_tree\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.81it/s]\n",
      "2024-08-06 20:03:31,288 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Cinderella reached her happy ending by attending the king's festival with the help of a little white bird who granted her wishes, wearing a beautiful dress and slippers thrown down to her by the bird, and capturing the heart of the prince who danced with her all night. The prince searched for her and found her, and they were married with the help of two white doves who announced that she was the true bride.\n"
     ]
    }
   ],
   "source": [
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Reproduce paper's experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NarrativeQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = R\"F:\\datasets\\reading_comprehension\\narrativeqa\\data\\test-00000-of-00008.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'Who is Mark Hunter?', 'tokens': ['Wh...</td>\n",
       "      <td>[{'text': 'He is a high school student in Phoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'Where does this radio station take p...</td>\n",
       "      <td>[{'text': 'It takes place in Mark's parents ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'Why do more students tune into Mark'...</td>\n",
       "      <td>[{'text': 'Mark talks about what goes on at sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'Who commits suicide?', 'tokens': ['W...</td>\n",
       "      <td>[{'text': 'Malcolm.', 'tokens': ['Malcolm', '....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'What does Paige jam into her microwa...</td>\n",
       "      <td>[{'text': 'She jams her medals and accolades. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'What does Mark do with his radio sta...</td>\n",
       "      <td>[{'text': 'He dismantles it and attaches it to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'What does Mark tell the protesting s...</td>\n",
       "      <td>[{'text': 'He tells them to make their own fut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'Who gets arrested?', 'tokens': ['Who...</td>\n",
       "      <td>[{'text': 'Mark and Nora.', 'tokens': ['Mark',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'What does the radio show cause?', 't...</td>\n",
       "      <td>[{'text': 'It causes trouble. ', 'tokens': ['I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'id': '0025577043f5090cd603c6aea60f26e2361955...</td>\n",
       "      <td>{'text': 'Where does Mark Broadcast his statio...</td>\n",
       "      <td>[{'text': 'Parent's Basement', 'tokens': ['Par...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "1  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "2  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "3  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "4  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "5  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "6  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "7  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "8  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "9  {'id': '0025577043f5090cd603c6aea60f26e2361955...   \n",
       "\n",
       "                                            question  \\\n",
       "0  {'text': 'Who is Mark Hunter?', 'tokens': ['Wh...   \n",
       "1  {'text': 'Where does this radio station take p...   \n",
       "2  {'text': 'Why do more students tune into Mark'...   \n",
       "3  {'text': 'Who commits suicide?', 'tokens': ['W...   \n",
       "4  {'text': 'What does Paige jam into her microwa...   \n",
       "5  {'text': 'What does Mark do with his radio sta...   \n",
       "6  {'text': 'What does Mark tell the protesting s...   \n",
       "7  {'text': 'Who gets arrested?', 'tokens': ['Who...   \n",
       "8  {'text': 'What does the radio show cause?', 't...   \n",
       "9  {'text': 'Where does Mark Broadcast his statio...   \n",
       "\n",
       "                                             answers  \n",
       "0  [{'text': 'He is a high school student in Phoe...  \n",
       "1  [{'text': 'It takes place in Mark's parents ba...  \n",
       "2  [{'text': 'Mark talks about what goes on at sc...  \n",
       "3  [{'text': 'Malcolm.', 'tokens': ['Malcolm', '....  \n",
       "4  [{'text': 'She jams her medals and accolades. ...  \n",
       "5  [{'text': 'He dismantles it and attaches it to...  \n",
       "6  [{'text': 'He tells them to make their own fut...  \n",
       "7  [{'text': 'Mark and Nora.', 'tokens': ['Mark',...  \n",
       "8  [{'text': 'It causes trouble. ', 'tokens': ['I...  \n",
       "9  [{'text': 'Parent's Basement', 'tokens': ['Par...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet(file_path)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'kind', 'url', 'file_size', 'word_count', 'start', 'end', 'summary', 'text'])\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[0, 'document'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 17:23:13,547 - Creating Leaf Nodes\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.05it/s]\n",
      "2024-08-07 17:23:42,992 - Created 137 Leaf Embeddings\n",
      "2024-08-07 17:23:42,992 - Building All Nodes\n",
      "2024-08-07 17:23:50,408 - Using Cluster TreeBuilder\n",
      "2024-08-07 17:23:53,042 - Constructing Layer 0\n",
      "2024-08-07 17:26:18,107 - Summarization Length: 100\n",
      "2024-08-07 17:27:06,997 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-07 17:27:48,437 - Node Texts Length: 291, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "2024-08-07 17:28:46,793 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-08-07 17:28:46,796 - Node Texts Length: 582, Summarized Text Length: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "2024-08-07 17:28:49,437 - HTTP Request: POST https://openai-20240510.openai.azure.com//openai/deployments/gpt-35-turbo-20240510/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 400 model_error\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mRA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\raptor\\RetrievalAugmentation.py:219\u001b[0m, in \u001b[0;36mRetrievalAugmentation.add_documents\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;66;03m# self.add_to_existing(docs)\u001b[39;00m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;241m=\u001b[39m TreeRetriever(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_retriever_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree)\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\raptor\\tree_builder.py:291\u001b[0m, in \u001b[0;36mTreeBuilder.build_from_text\u001b[1;34m(self, text, use_multithreading)\u001b[0m\n\u001b[0;32m    287\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding All Nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m all_nodes \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(leaf_nodes)\n\u001b[1;32m--> 291\u001b[0m root_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_to_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m tree \u001b[38;5;241m=\u001b[39m Tree(all_nodes, root_nodes, leaf_nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, layer_to_nodes)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\raptor\\cluster_tree_builder.py:130\u001b[0m, in \u001b[0;36mClusterTreeBuilder.construct_tree\u001b[1;34m(self, current_level_nodes, all_tree_nodes, layer_to_nodes, use_multithreading)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m clusters:\n\u001b[1;32m--> 130\u001b[0m         \u001b[43mprocess_cluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnew_level_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnext_node_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43msummarization_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m         next_node_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    139\u001b[0m layer_to_nodes[layer \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(new_level_nodes\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\raptor\\cluster_tree_builder.py:77\u001b[0m, in \u001b[0;36mClusterTreeBuilder.construct_tree.<locals>.process_cluster\u001b[1;34m(cluster, new_level_nodes, next_node_index, summarization_length, lock)\u001b[0m\n\u001b[0;32m     69\u001b[0m node_texts \u001b[38;5;241m=\u001b[39m get_text(cluster)\n\u001b[0;32m     71\u001b[0m summarized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummarize(\n\u001b[0;32m     72\u001b[0m     context\u001b[38;5;241m=\u001b[39mnode_texts,\n\u001b[0;32m     73\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39msummarization_length,\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     76\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode Texts Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(node_texts))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Summarized Text Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummarized_text\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m __, new_parent_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[0;32m     81\u001b[0m     next_node_index, summarized_text, {node\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m cluster}\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lock:\n",
      "File \u001b[1;32me:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\tiktoken\\core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "RA.add_documents(data.loc[0, 'document']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = RA.answer_question(\n",
    "    question=data.loc[0, 'question']['text']\n",
    ")\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\yi\\NUSTM\\project\\FinQA4\\raptor\\models\\unifiedqa-v2-t5-3b-1363200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]e:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading 'README.md' to 'models\\unifiedqa-v2-t5-3b-1363200\\.cache\\huggingface\\download\\README.md.f1e8a245914bdae4cf5f4b306bf372307e85c2f9.incomplete'\n",
      "Downloading 'spiece.model' to 'models\\unifiedqa-v2-t5-3b-1363200\\.cache\\huggingface\\download\\spiece.model.d60acb128cf7b7f2536e8f38a5b18a05535c9e14c7a355904270e15b0945ea86.incomplete'\n",
      "Downloading 'tokenizer_config.json' to 'models\\unifiedqa-v2-t5-3b-1363200\\.cache\\huggingface\\download\\tokenizer_config.json.be4b890163aab9634b49284ab988e39b78d99015.incomplete'\n",
      "Downloading 'special_tokens_map.json' to 'models\\unifiedqa-v2-t5-3b-1363200\\.cache\\huggingface\\download\\special_tokens_map.json.881bdbffc06e471924ecea57f962bc5f8e2a9f21.incomplete'\n",
      "Downloading 'generation_config.json' to 'models\\unifiedqa-v2-t5-3b-1363200\\.cache\\huggingface\\download\\generation_config.json.d52815623b46b7db1c4b957b5a83a8ad30b0146a.incomplete'\n",
      "Downloading '.gitattributes' to 'models\\unifiedqa-v2-t5-3b-1363200\\.cache\\huggingface\\download\\.gitattributes.6d34772f5ca361021038b404fb913ec8dc0b1a5a.incomplete'\n",
      "Downloading 'config.json' to 'models\\unifiedqa-v2-t5-3b-1363200\\.cache\\huggingface\\download\\config.json.997f0881d153bac42aedc7f798c16e6b4e30312d.incomplete'\n",
      "Downloading 'pytorch_model.bin' to 'models\\unifiedqa-v2-t5-3b-1363200\\.cache\\huggingface\\download\\pytorch_model.bin.56ca96486af48612d40dcb6a16a4ba886c0e445d2b561a7203b3e03a91a7216e.incomplete'\n",
      "Download complete. Moving file to models\\unifiedqa-v2-t5-3b-1363200\\README.md\n",
      "Download complete. Moving file to models\\unifiedqa-v2-t5-3b-1363200\\tokenizer_config.json\n",
      "Download complete. Moving file to models\\unifiedqa-v2-t5-3b-1363200\\special_tokens_map.json\n",
      "Download complete. Moving file to models\\unifiedqa-v2-t5-3b-1363200\\generation_config.json\n",
      "Download complete. Moving file to models\\unifiedqa-v2-t5-3b-1363200\\.gitattributes\n",
      "\n",
      "Fetching 8 files:  12%|█▎        | 1/8 [00:00<00:06,  1.13it/s]Download complete. Moving file to models\\unifiedqa-v2-t5-3b-1363200\\config.json\n",
      "\n",
      "Fetching 8 files:  38%|███▊      | 3/8 [00:01<00:01,  3.23it/s]Download complete. Moving file to models\\unifiedqa-v2-t5-3b-1363200\\spiece.model\n",
      "Error while downloading from https://cdn-lfs.hf-mirror.com/allenai/unifiedqa-v2-t5-3b-1363200/56ca96486af48612d40dcb6a16a4ba886c0e445d2b561a7203b3e03a91a7216e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1723273801&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzI3MzgwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9hbGxlbmFpL3VuaWZpZWRxYS12Mi10NS0zYi0xMzYzMjAwLzU2Y2E5NjQ4NmFmNDg2MTJkNDBkY2I2YTE2YTRiYTg4NmMwZTQ0NWQyYjU2MWE3MjAzYjNlMDNhOTFhNzIxNmU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=AI6vi6yVp5e%7EreL6n6iM4PxZwkrf1nvgg4GTGqu0JB5EIOd5H25bOZe%7E1vv91h4RREsklwUYnpW4RYK9a6K8OIjVJRmEQmHhMYW-f4eYQgU4JDmraZu2glgj41wxisw-pPWWfUuSYzUt4%7EHr-uTcT1rC8B3FwKEgMvGYF1o3pHUyA8Gi2dTy46YrCtd26Q2u9uRsAMfMFavCsPQm3a1odSOi%7EyuD42UVmWABSlF8GwjJY6PvbZfrMkGS0mhqohqpgGAWxmRT9KUCkUysvEHc22KSFMm9trlWuNncPtSzInfMdVPBpKY4R7BQqvHDSR0qHOilopZfy%7Ehm428CskL-2w__&Key-Pair-Id=K3ESJI6DHPFC7: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.hf-mirror.com/allenai/unifiedqa-v2-t5-3b-1363200/56ca96486af48612d40dcb6a16a4ba886c0e445d2b561a7203b3e03a91a7216e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1723273801&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzI3MzgwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9hbGxlbmFpL3VuaWZpZWRxYS12Mi10NS0zYi0xMzYzMjAwLzU2Y2E5NjQ4NmFmNDg2MTJkNDBkY2I2YTE2YTRiYTg4NmMwZTQ0NWQyYjU2MWE3MjAzYjNlMDNhOTFhNzIxNmU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=AI6vi6yVp5e%7EreL6n6iM4PxZwkrf1nvgg4GTGqu0JB5EIOd5H25bOZe%7E1vv91h4RREsklwUYnpW4RYK9a6K8OIjVJRmEQmHhMYW-f4eYQgU4JDmraZu2glgj41wxisw-pPWWfUuSYzUt4%7EHr-uTcT1rC8B3FwKEgMvGYF1o3pHUyA8Gi2dTy46YrCtd26Q2u9uRsAMfMFavCsPQm3a1odSOi%7EyuD42UVmWABSlF8GwjJY6PvbZfrMkGS0mhqohqpgGAWxmRT9KUCkUysvEHc22KSFMm9trlWuNncPtSzInfMdVPBpKY4R7BQqvHDSR0qHOilopZfy%7Ehm428CskL-2w__&Key-Pair-Id=K3ESJI6DHPFC7: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Download complete. Moving file to models\\unifiedqa-v2-t5-3b-1363200\\pytorch_model.bin\n",
      "\n",
      "Fetching 8 files:  62%|██████▎   | 5/8 [09:41<07:27, 149.26s/it]\n",
      "Fetching 8 files: 100%|██████████| 8/8 [09:41<00:00, 72.64s/it] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
    "!huggingface-cli download --resume-download sentence-transformers/multi-qa-mpnet-base-cos-v1 --local-dir ./models/multi-qa-mpnet-base-cos-v1\n",
    "\n",
    "# https://huggingface.co/allenai/unifiedqa-v2-t5-3b-1363200\n",
    "!huggingface-cli download --resume-download allenai/unifiedqa-v2-t5-3b-1363200 --local-dir ./models/unifiedqa-v2-t5-3b-1363200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\datasets\\reading_comprehension\\narrativeqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\yi\\NUSTM\\project\\FinQA4\\raptor\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\n",
      "Fetching 39 files:   0%|          | 0/39 [00:00<?, ?it/s]Downloading 'data/narrativeqa-master.zip' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\narrativeqa-master.zip.d9fc92d5f53409f845ba44780e6689676d879c739589861b4805064513d1476b.incomplete'\n",
      "Downloading 'data/train-00002-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00002-of-00024.parquet.d876d0df116532d476e2cf9b0a6f22d3844b9a2837a66ff431fe5473b9651acb.incomplete'\n",
      "Downloading 'data/train-00003-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00003-of-00024.parquet.beef361d669198b2ca60b3906df894474bfc29ab86e3ed5b901deaf818bc1034.incomplete'\n",
      "Downloading 'data/train-00001-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00001-of-00024.parquet.d6916659f26644945dcdbcba0cb153af8ce42c2482b6e6ee455af8a03aae0287.incomplete'\n",
      "Downloading 'data/train-00006-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00006-of-00024.parquet.6a7d3c25d3185c3c0df8272664494fbc53b78e5c79f53efc4bf620aee36a9495.incomplete'\n",
      "Downloading 'data/train-00004-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00004-of-00024.parquet.66703341abb91e44300106d7fd0c87e9a4d86b1e6bbd33ebe855fc557f73126f.incomplete'\n",
      "Downloading 'data/train-00007-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00007-of-00024.parquet.a33131a90ca1de261125065d66f6a2bd9a4403f34b3d2d525e77488ad905f54d.incomplete'\n",
      "Downloading 'data/train-00005-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00005-of-00024.parquet.162781ba93739daad62786144572cd8ff3c77b63ca1a1c0c788c02d74548aeba.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\narrativeqa-master.zip\n",
      "\n",
      "Fetching 39 files:   8%|▊         | 3/39 [00:03<00:38,  1.07s/it]Downloading 'data/train-00008-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00008-of-00024.parquet.207b66118655892930c495da7b7a036c03d167b57b248a7c27bdfd937e1be29c.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00008-of-00024.parquet\n",
      "Downloading 'data/train-00009-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00009-of-00024.parquet.4b36dc10fd6a1f2d66ca3b26705c1a98ceb0c7c8ea0c99e91b168a59fd86d357.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00004-of-00024.parquet\n",
      "Downloading 'data/train-00010-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00010-of-00024.parquet.abc135b05617d6e627197b2d43715a7e8f1ea2262d750c9b2d2b23c152e841f2.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00003-of-00024.parquet\n",
      "Downloading 'data/train-00011-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00011-of-00024.parquet.674c0876d1f2f60ef1f3b8f0d7d48d0f731baefd59d12c4b2b7ab715a5ad301f.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00006-of-00024.parquet\n",
      "Downloading 'data/train-00012-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00012-of-00024.parquet.66253913e5c161d37eef2d5212e54ae26f63ddd73dac1966dd3a55800e16a516.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00002-of-00024.parquet\n",
      "Downloading 'data/train-00013-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00013-of-00024.parquet.edfb135333a47a9c7a307af8d01474a598bf2db307e16b9be026f375ed972363.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00011-of-00024.parquet\n",
      "Downloading 'data/train-00014-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00014-of-00024.parquet.67c5b1dcc311b2a12c102f32d2f49758a97d7928cb578527cf3df2242e3c6e5c.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00014-of-00024.parquet\n",
      "Downloading 'data/train-00015-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00015-of-00024.parquet.3317553c52f6093ee3a36b94861d9de5c4b34d2adf4ebbd38fb0072974b6125c.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00009-of-00024.parquet\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00013-of-00024.parquet\n",
      "Downloading 'data/train-00016-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00016-of-00024.parquet.1eda7b63549691eaffb613afbb5f87f1f65a74f4562546e9f6e4afb86c2e62b8.incomplete'\n",
      "Downloading 'data/train-00017-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00017-of-00024.parquet.3f0b1b763c5b7fbed56b6a264cee4b2bff4ad4a84abe6fdee3282eca434cbddd.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00017-of-00024.parquet\n",
      "Downloading 'data/train-00018-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00018-of-00024.parquet.48183a438c60d8fe80a1fab74f871db3a3ec2de3799607aaa791940272acab99.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00001-of-00024.parquet\n",
      "\n",
      "Fetching 39 files:  36%|███▌      | 14/39 [01:00<01:53,  4.54s/it]Downloading 'data/train-00019-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00019-of-00024.parquet.9edd6ce0ddb59e9e71424d2441a22ea5afdc64459efb3337e54d2096589f86a7.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00018-of-00024.parquet\n",
      "Downloading 'data/train-00020-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00020-of-00024.parquet.7348a1895950b08039f72abadcede635886b0af1e9ee33a79a5bd5af7123542e.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00005-of-00024.parquet\n",
      "\n",
      "Fetching 39 files:  46%|████▌     | 18/39 [01:04<01:12,  3.47s/it]Downloading 'data/train-00021-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00021-of-00024.parquet.198cca049bb959dd668920ce9ac3b14e68ba9537da9824dd49b52fcbe4f04a30.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00020-of-00024.parquet\n",
      "Downloading 'data/train-00022-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00022-of-00024.parquet.9d5333d8d0132bc3cf7662f4c491c4ab00b29e1562c04f444b3e8fb01478aeef.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00022-of-00024.parquet\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00007-of-00024.parquet\n",
      "\n",
      "Fetching 39 files:  51%|█████▏    | 20/39 [01:07<00:59,  3.15s/it]Downloading 'data/train-00023-of-00024.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\train-00023-of-00024.parquet.cac81336b130e9265f11f508825e7c9f16c247f9bb09e2893a16d78b04490c7b.incomplete'\n",
      "Downloading 'data/validation-00000-of-00003.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\validation-00000-of-00003.parquet.5bd4cfb854ec4c22b98583fa4b6b16f5e6f7461d8bfc13cfe8e7eb12b30871d0.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00012-of-00024.parquet\n",
      "Downloading 'data/validation-00001-of-00003.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\validation-00001-of-00003.parquet.2f0728b3f819c1ccf67a1d0e1c37bfe8523063a46a5742275770d3db2c2ed799.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\validation-00000-of-00003.parquet\n",
      "Downloading 'data/validation-00002-of-00003.parquet' to 'F:\\datasets\\reading_comprehension\\narrativeqa\\.cache\\huggingface\\download\\data\\validation-00002-of-00003.parquet.3f6da16ec6688f788947023d84408bb7af71de7fd379d86b4ea517c42297d39b.incomplete'\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00010-of-00024.parquet\n",
      "\n",
      "Fetching 39 files:  59%|█████▉    | 23/39 [01:14<00:46,  2.93s/it]Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00015-of-00024.parquet\n",
      "\n",
      "Fetching 39 files:  72%|███████▏  | 28/39 [01:15<00:20,  1.85s/it]Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\validation-00002-of-00003.parquet\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00023-of-00024.parquet\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00021-of-00024.parquet\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00016-of-00024.parquet\n",
      "\n",
      "Fetching 39 files:  74%|███████▍  | 29/39 [01:30<00:30,  3.10s/it]Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\validation-00001-of-00003.parquet\n",
      "Download complete. Moving file to F:\\datasets\\reading_comprehension\\narrativeqa\\data\\train-00019-of-00024.parquet\n",
      "\n",
      "Fetching 39 files:  82%|████████▏ | 32/39 [02:21<00:51,  7.41s/it]\n",
      "Fetching 39 files: 100%|██████████| 39/39 [02:21<00:00,  3.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Download datasets\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# narrative-qa: https://huggingface.co/datasets/deepmind/narrativeqa\n",
    "!huggingface-cli download --repo-type dataset --resume-download deepmind/narrativeqa --local-dir F:/datasets/reading_comprehension/narrativeqa\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
